{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gumbel_softmax(logits, temperature=1.0, hard=False):\n",
    "#     y_soft = gumbel_softmax_sample(logits, temperature)\n",
    "#     if hard:\n",
    "#         index = y_soft.max(dim=-1, keepdim=True)[1]\n",
    "#         y_hard = torch.zeros_like(y_soft).scatter_(-1, index, 1.0)\n",
    "#         return (y_hard - y_soft).detach() + y_soft\n",
    "#     else:\n",
    "#         return y_soft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb43906",
   "metadata": {},
   "source": [
    "**def gumbel_softmax(logits, temperature=1.0, hard=False):**\n",
    "- logits: tensor con i punteggi grezzi delle opzioni.\n",
    "- temperature: quanto deve essere netta o morbida la scelta (default: 1.0).\n",
    "- hard: se True, vogliamo un output “netto” (come argmax), ma differenziabile.\n",
    "- hard=False significa che il parametro hard ha un valore predefinito.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "\n",
    "**y_soft = gumbel_softmax_sample(logits, temperature)**\n",
    "-   Risultato: y_soft è un vettore di probabilità morbide.\n",
    "\n",
    "esempio:\n",
    "\n",
    "logits = [2.0, 1.0, 0.1]\n",
    "\n",
    "y_soft = [0.78, 0.17, 0.05]\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "        nel caso di scelta netta :         \n",
    "\n",
    "\n",
    "**if hard: index = y_soft.max(dim=-1, keepdim=True)[1]**\n",
    "\n",
    "-   y_soft.max(...) restituisce il valore massimo e il suo indice.\n",
    "-   dim=-1: lavora sull’ultima dimensione (di solito su righe).\n",
    "-   [1]: prende solo l’indice\\posizione (non il valore).\n",
    "-   keepdim=True: mantiene la stessa forma (utile dopo per .scatter_).\n",
    "\n",
    "\n",
    "\n",
    "**y_hard = torch.zeros_like(y_soft).scatter_(-1, index, 1.0)**\n",
    "\n",
    "-   torch.zeros_like(y_soft): crea un vettore di zeri.\n",
    "-   d.scatter_: imposta a 1.0 la posizione index.\n",
    "-   Risultato: un one-hot vector, es: [1.0, 0.0, 0.0]\n",
    "\n",
    "\n",
    "\n",
    "**return (y_hard - y_soft).detach() + y_soft**\n",
    "\n",
    "-   Questo è il famoso trucco del \"straight-through estimator\".\n",
    "-   y_hard - y_soft:Differenza tra il vettore netto e morbido\n",
    "-   detach(): Spezza il gradiente: non aggiorna y_hard\n",
    "-   + y_soft: Mantiene il gradiente solo su y_soft\n",
    "\n",
    "\n",
    "                \"Scegli una sola opzione, ma fai finta di essere continuo così il modello può ancora imparare.\"               \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sono io a scelgiere quando attivare  o meno la hard hoice.\n",
    "\n",
    "        Quando hard=False (default) == Il modello impara a distribuire l’attenzione tra le scelte.\n",
    "Usi un output soft (valori continui).\n",
    "Buono per modelli che hanno bisogno di probabilità (es: classificazione, policy probabilistiche).\n",
    "I valori sono tutti >0 e sommano a 1.\n",
    "\n",
    "        Quando hard=True == Il modello sceglie una sola opzione, come farebbe un argmax, ma con una scelta che ha imparato.\n",
    "Usi un output one-hot, cioè una sola scelta attiva.\n",
    "Serve quando vuoi che il modello selezioni una cosa sola (es: una feature, un'azione, un token).\n",
    "Ma vuoi comunque poter fare backpropagation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
